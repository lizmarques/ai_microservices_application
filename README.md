# The Efficiency of Microservices Architecture for AI Applications

A comprehensive microservices-based platform that demonstrates the efficiency of microservices architecture for AI applications, providing Speech-to-Text (STT), Large Language Model (LLM), and Text-to-Speech (TTS) capabilities. The application is fully containerized using Docker and includes monitoring, load balancing, and performance testing to demonstrate architectural efficiency.

> **Note**: While the main goal is to evaluate the efficiency of microservices architecture for AI applications, a user-friendly Streamlit frontend has been built to allow people to interact with the application and experience the complete AI workflow in real-time.

## üé• Application Demo

## üèóÔ∏è Architecture Overview

<p align="center">
  <img src="./images/application_architecture_v3.PNG" alt="Application Microservices Architecture" width="70%" /><br>
  <em>Figure 1 ‚Äì Microservices Architecture of the Application</em>
</p>

The platform consists of the following microservices:

- **STT Service**: Converts audio files to text using OpenAI Whisper
- **LLM Service**: Processes text queries using Qwen2-72B-Instruct model
- **TTS Service**: Converts text to speech using Google Text-to-Speech (gTTS)
- **Frontend (Web App)**: Streamlit-based web interface for user interaction
- **API Gateway**: Traefik for load balancing and routing
- **Message Queue**: Redis for task queuing and Celery for asynchronous processing
- **Databases**: PostgreSQL for STT/TTS data, MongoDB for LLM data
- **Monitoring**: Prometheus and Grafana for observability
- **Load Testing**: Locust for performance testing

## üéØ **AI Models Used**

### **Speech-to-Text (STT)**: OpenAI Whisper
- **Model**: Whisper Base
- **Capabilities**: Automatic speech recognition with high accuracy
- **Features**: Multilingual support, noise robustness, real-time transcription
- **Use Case**: Converts audio input to text for further processing

### **Large Language Model (LLM)**: Qwen 2.5 72B Instruct
- **Model**: Qwen/Qwen2.5-72B-Instruct
- **Capabilities**: Advanced reasoning, multilingual understanding, instruction following
- **Features**: 72B parameters, optimized for instruction-following tasks
- **Use Case**: Processes transcribed text and generates intelligent responses

### **Text-to-Speech (TTS)**: Google Text-to-Speech (gTTS)
- **Model**: Google's Text-to-Speech API
- **Capabilities**: Natural-sounding speech synthesis
- **Features**: Multilingual support, customizable voice parameters
- **Use Case**: Converts LLM responses back to audio for user interaction

## üîÑ **Complete AI Workflow**

The application demonstrates a complete AI conversation pipeline:

1. **üé§ Audio Input**: User records audio via the Streamlit frontend interface
2. **üìù STT Processing**: Audio is sent to the STT service (Whisper) for transcription
3. **üß† LLM Processing**: Transcribed text is processed by the LLM service (Qwen 2.5 72B)
4. **üîä TTS Generation**: LLM response is converted to speech using gTTS
5. **üéµ Audio Output**: Generated audio is automatically played back to the user

### **User Experience**
- **Interactive Interface**: Clean, intuitive Streamlit web application
- **Real-time Processing**: Asynchronous task processing with status updates
- **Seamless Integration**: Complete AI conversation in a single workflow
- **Performance Monitoring**: Built-in metrics and monitoring for efficiency analysis

## üöÄ Quick Start

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/lizmarques/ai-microservices-application.git
   ```

2. **Start the application:**
   ```bash
   docker-compose up -d
   ```

3. **Verify all services are running:**
   ```bash
   docker-compose ps
   ```
4. **Rebuild specific services:**
   ```bash
   # Rebuild STT service
   docker-compose build stt
   
   # Rebuild all services
   docker-compose build
   
   # Rebuild and restart
   docker-compose up --build
   ```
5. **Access the application:**
   - **Main Application (Streamlit Frontend)**: http://localhost:443
   - **Grafana Monitoring**: http://localhost:3000
   - **Prometheus Metrics**: http://localhost:9090
   - **PgAdmin**: http://localhost:5050

## üìã Service Details

### Core Services

| Service | Port | Description | Database |
|---------|------|-------------|----------|
| STT API | 8502 | Speech-to-Text processing | PostgreSQL (stt_service) |
| LLM API | 8503 | Language model processing | MongoDB (llm_service) |
| TTS API | 8504 | Text-to-Speech generation | PostgreSQL (tts_service) |
| Frontend | 8505 | Streamlit web interface | - |
| Traefik | 80/443 | API Gateway & Load Balancer | - |

### Infrastructure Services

| Service | Port | Description |
|---------|------|-------------|
| Redis | 6379 | Message broker for Celery |
| PostgreSQL STT | 5433 | Database for STT data |
| PostgreSQL TTS | 5434 | Database for TTS data |
| MongoDB | 27017 | Database for LLM data |
| PgAdmin | 5050 | PostgreSQL administration |
| Prometheus | 9090 | Metrics collection |
| Grafana | 3000 | Monitoring dashboard |

## üìä API Endpoints

### STT Service
- `POST /stt` - Upload audio file for transcription
- `GET /task_status_stt/{task_id}` - Check transcription status
- `GET /metrics` - Prometheus metrics

### LLM Service
- `POST /llm` - Send text for LLM processing
- `GET /task_status_llm/{task_id}` - Check LLM processing status
- `GET /metrics` - Prometheus metrics

### TTS Service
- `POST /tts` - Send text for speech generation
- `GET /task_status_tts/{task_id}` - Check TTS generation status
- `GET /get_audio/{filename}` - Download generated audio file
- `GET /metrics` - Prometheus metrics

## üîß Configuration

### Environment Variables

The application uses environment variables for configuration. Key variables include:

- `POSTGRES_DB`: Database name
- `POSTGRES_USER`: Database username
- `POSTGRES_PASSWORD`: Database password
- `POSTGRES_HOST`: Database host
- `MONGO_HOST`: MongoDB host
- `CELERY_BROKER_URL`: Redis broker URL

### Database Configuration

- **STT Service**: Uses PostgreSQL with `stt_transcriptions` table
- **TTS Service**: Uses PostgreSQL with `tts_results` table  
- **LLM Service**: Uses MongoDB with `llm_queries` collection

## üß™ Load Testing & Monitoring

### Load Testing with Locust

The application includes comprehensive load testing capabilities:

1. **Start load testing:**
   ```bash
   docker-compose up locust
   ```

2. **Access Locust UI:**
   - Navigate to http://localhost:8089
   - Configure test parameters (users, spawn rate, duration)
   - Run performance tests

3. **Test Scenarios:**
   - **Scenario 1**: Light load (10 users, 1 user/second, 5 minutes)
   - **Scenario 2**: Heavy load (50+ users, stress testing)

## üéØ **Research Focus**

This project serves as a comprehensive study of microservices architecture efficiency for AI applications, featuring:

- **Architectural Analysis**: Performance comparison between monolithic vs. microservices approaches
- **Load Testing**: Comprehensive testing scenarios using Locust for performance evaluation
- **Monitoring & Metrics**: Real-time monitoring with Prometheus and Grafana
- **Scalability Studies**: Analysis of service scaling and resource utilization
- **Efficiency Metrics**: Detailed performance measurements and optimization insights

## üß™ Experimental Methodology & Results

This research follows an **exploratory and quantitative experimental design**, evaluating the **efficiency and scalability** of a real-time AI application based on a **microservices architecture**.  
The system enables human‚Äìmachine interaction through live speech, leveraging three core services:

- **STT (Speech-to-Text)** ‚Äì transcribes the user‚Äôs audio input  
- **LLM (Large Language Model)** ‚Äì generates textual responses  
- **TTS (Text-to-Speech)** ‚Äì converts text back into synthesized speech  

Each service runs independently and communicates through REST APIs managed by an **API Gateway** (Traefik). Every microservice has its own database‚Äî**PostgreSQL** for structured data and **MongoDB** for LLM responses‚Äîfollowing the *database-per-service* pattern to ensure autonomy and isolation.  
All services are containerized via **Docker Compose**, allowing horizontal and vertical scalability.

---

### ‚öôÔ∏è Experimental Setup

Performance tests were executed with **Locust**, simulating real-time requests to measure:
- Total pipeline latency (STT ‚Üí LLM ‚Üí TTS)  
- Individual service latency  
- Throughput (Requests per Second ‚Äì RPS)  
- Error rate and stability under load  

Two controlled scenarios were evaluated using the same input prompt in portuguese, **‚ÄúQuem fundou a Apple? (Who founded Apple?)‚Äù**, to ensure consistency.

---

## üìà Performance Results

### **Scenario 1 ‚Äì Low Load (10 Concurrent Users)**

This baseline test evaluated latency and overhead under light traffic.  
The system processed requests smoothly, maintaining an average total latency **below 1.5 seconds** with **no failed requests**.  
The API Gateway introduced minimal overhead (<100ms), and REST communication proved efficient.

<p align="center">
  <img src="./images/graph_s1_rps_v3.PNG" alt="Total Requests per Second ‚Äì Scenario 1" /><br>
  <em>Figure 2 ‚Äì Total Requests per Second (Low Load)</em>
</p>

<br><br>

<p align="center">
  <img src="./images/graph_s1_latency_v3.PNG" alt="Response Times ‚Äì Scenario 1" /><br>
  <em>Figure 3 ‚Äì Response Times (Low Load)</em>
</p>


‚úÖ **Key Findings**  
- Consistent latency and stability across all services  
- No observed bottlenecks  
- REST-based communication and API Gateway overhead negligible  

---

### **Scenario 2 ‚Äì High Load (500 Concurrent Users)**

The stress test simulated high concurrency to evaluate scalability and fault tolerance.  
Throughput peaked at **~30 requests per second**, stabilizing after approximately 3 minutes.  
However, **STT** emerged as the main bottleneck due to higher computational demands, resulting in increased latency and temporary saturation.

<p align="center">
  <img src="./images/graph_s2_rps_v3.PNG" alt="Total Requests per Second ‚Äì Scenario 2" /><br>
  <em>Figure 4 ‚Äì Total Requests per Second (High Load)</em>
</p>

<br><br>

<p align="center">
  <img src="./images/graph_s2_latency_v3.PNG" alt="Response Times ‚Äì Scenario 2" /><br>
  <em>Figure 5 ‚Äì Response Times (High Load)</em>
</p>

<br><br>

<p align="center">
  <img src="./images/graph_s2_failures_v3.png" alt="Failures per Endpoint" /><br>
  <em>Figure 6 ‚Äì Total Failures per Endpoint (High Load)</em>
</p>

‚úÖ **Key Findings**  
- The **STT** service reached latency peaks (~10s) during initial load spikes, confirming it as the main computational bottleneck  
- The **LLM** and **TTS** services maintained stable performance  
- The **API Gateway** added slight latency overhead under heavy load but remained operational  
- The system sustained **~30 RPS** after stabilization, validating the architecture‚Äôs scalability potential  

---

### üìä Summary

| Metric | Scenario 1 (Low Load) | Scenario 2 (High Load) |
|:--|:--:|:--:|
| Average Pipeline Latency | < 1.5 s | 3‚Äì4 s (after stabilization) |
| API Gateway Overhead | < 100 ms | < 250 ms |
| Throughput | 1‚Äì3 RPS | ~30 RPS |
| Success Rate | 100% | 95‚Äì97% (temporary errors during spikes) |
| Main Bottleneck | None | STT service |

---

### üß† Interpretation

- The **modular microservices architecture** proved efficient and resilient under both light and heavy load conditions.  
- **Independent databases and containerization** contributed to isolation and recoverability.  
- **STT performance** directly influenced total pipeline latency, suggesting optimization or asynchronous queuing strategies (e.g., Celery) could further enhance scalability.

## üõ†Ô∏è Development

### Project Structure

```
‚îú‚îÄ‚îÄ services/                 # Microservices APIs
‚îÇ   ‚îú‚îÄ‚îÄ stt_api.py          # Speech-to-Text service (Whisper)
‚îÇ   ‚îú‚îÄ‚îÄ llm_api.py          # Language model service (Qwen 2.5 72B)
‚îÇ   ‚îú‚îÄ‚îÄ tts_api.py          # Text-to-Speech service (gTTS)
‚îÇ   ‚îú‚îÄ‚îÄ celery_worker.py    # Celery worker configuration
‚îÇ   ‚îú‚îÄ‚îÄ requirements_stt.txt # STT service dependencies
‚îÇ   ‚îú‚îÄ‚îÄ requirements_llm.txt # LLM service dependencies
‚îÇ   ‚îú‚îÄ‚îÄ requirements_tts.txt # TTS service dependencies
‚îÇ   ‚îî‚îÄ‚îÄ requirements_worker.txt # Worker dependencies
‚îú‚îÄ‚îÄ frontend/               # Streamlit web interface
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py   # Main Streamlit application
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile         # Frontend container configuration
‚îÇ   ‚îî‚îÄ‚îÄ requirements_streamlit.txt # Frontend dependencies
‚îú‚îÄ‚îÄ Docker/                 # Docker configurations
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile-stt     # STT service container
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile-llm     # LLM service container
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile-tts     # TTS service container
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile-worker  # Worker service container
‚îú‚îÄ‚îÄ locust/                # Load testing
‚îÇ   ‚îú‚îÄ‚îÄ locustfile.py      # Locust test scenarios
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile-locust  # Locust container
‚îÇ   ‚îî‚îÄ‚îÄ requirements_locust.txt # Locust dependencies
‚îú‚îÄ‚îÄ monitoring/             # Monitoring configuration
‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml
‚îú‚îÄ‚îÄ traefik/               # API Gateway configuration
‚îÇ   ‚îî‚îÄ‚îÄ traefik.yml
‚îú‚îÄ‚îÄ shared/                # Shared utilities
‚îÇ   ‚îî‚îÄ‚îÄ celery_config.py
‚îú‚îÄ‚îÄ docker-compose.yml     # Main orchestration
‚îî‚îÄ‚îÄ README.md             # This documentation
```
